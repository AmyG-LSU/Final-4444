{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Crime Data:\n",
    "    Row= parishes\n",
    "    Column= Month - Year"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "crime_Data = pd.read_csv('data/Crime Data Month Year.csv')\n",
    "crime_Data.head()\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "School Data:\n",
    "\n"
   ],
   "id": "b1b0a3ece31d9e49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "columns = [2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022, 2023, 2024]\n",
    "\n",
    "parishes = [\n",
    "    \"Acadia\", \"Allen\", \"Ascension\", \"Assumption\", \"Avoyelles\", \"Beauregard\", \"Bienville\", \"Bossier\",\n",
    "    \"Caddo\", \"Calcasieu\", \"Caldwell\", \"Cameron\", \"Catahoula\", \"Claiborne\", \"Concordia\", \"DeSoto\",\n",
    "    \"East Baton Rouge\", \"East Carroll\", \"East Feliciana\", \"Evangeline\", \"Franklin\", \"Grant\", \"Iberia\",\n",
    "    \"Iberville\", \"Jackson\", \"Jefferson\", \"Jefferson Davis\", \"Lafayette\", \"Lafourche\", \"LaSalle\",\n",
    "    \"Lincoln\", \"Livingston\", \"Madison\", \"Morehouse\", \"Natchitoches\", \"Orleans\", \"Ouachita\",\n",
    "    \"Plaquemines\", \"Pointe Coupee\", \"Rapides\", \"Red River\", \"Richland\", \"Sabine\", \"St. Bernard\",\n",
    "    \"St. Charles\", \"St. Helena\", \"St. James\", \"St. John the Baptist\", \"St. Landry\", \"St. Martin\",\n",
    "    \"St. Mary\", \"St. Tammany\", \"Tangipahoa\", \"Tensas\", \"Terrebonne\", \"Union\", \"Vermilion\", \"Vernon\",\n",
    "    \"Washington\", \"Webster\", \"West Baton Rouge\", \"West Carroll\", \"West Feliciana\", \"Winn\"\n",
    "]\n",
    "\n",
    "folder_path = \"data/School Data Year\"\n",
    "\n",
    "# Create empyty DF with column for years and rows as parishes\n",
    "school_data = pd.DataFrame(index=parishes, columns=columns)\n",
    "\n",
    "# Sort file names so years are consistent\n",
    "items_in_folder = sorted(os.listdir(folder_path))\n",
    "\n",
    "for item_name in items_in_folder:\n",
    "    item_full_path = os.path.join(folder_path, item_name)\n",
    "\n",
    "    if os.path.isfile(item_full_path):\n",
    "\n",
    "        try:\n",
    "            year = int(\"\".join(filter(str.isdigit, item_name))[:4])\n",
    "            if year not in school_data.columns:\n",
    "                print(f\"‚ö†Ô∏è {item_name} skipped ‚Äî year {year} not in template\")\n",
    "            else:\n",
    "                df = pd.read_excel(item_full_path)\n",
    "                # Only waNT THE COLUMN NAMED DPS AND THE ROWS 2 AND 64\n",
    "                schoolfiltered = df.loc[0:63,\"DPS\"]\n",
    "\n",
    "                # add the grade number to the appropriate year column\n",
    "                school_data[year] = schoolfiltered.values\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {item_name}: {e}\")\n",
    "\n",
    "school_data.head()"
   ],
   "id": "75cf03b21a5b9a72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Home Mortgage Rates:\n",
    "\n"
   ],
   "id": "87f8876b31ac9312"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HomeRatesUnfiltered = pd.read_excel(\"data/Home Mortgage Rates.xlsx\")\n",
    "HomeRatesUnfiltered[\"observation_date\"] = pd.to_datetime(HomeRatesUnfiltered[\"observation_date\"])\n",
    "\n",
    "# Resample first (needs datetime index)\n",
    "HomeRates_Data = (\n",
    "    HomeRatesUnfiltered\n",
    "        .set_index(\"observation_date\")\n",
    "        .resample(\"M\")[\"MORTGAGE30US\"]\n",
    "        .mean()\n",
    ")\n",
    "\n",
    "# Convert end-of-month dates to YYYY-MM\n",
    "HomeRates_Data.index = HomeRates_Data.index.to_period(\"M\").astype(str)\n",
    "HomeRates_Data = HomeRates_Data.to_frame().T\n",
    "\n",
    "HomeRates_Data.head()\n"
   ],
   "id": "9c75e0732792ea4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Home Values:",
   "id": "21e48f513976dd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HomeValuesUnfilterd = pd.read_csv(\"data/Home Values Month Year.csv\")\n",
    "HomeValuesUnfilterd[\"RegionName\"] = HomeValuesUnfilterd[\"RegionName\"].str.replace(\" Parish\", \"\", regex=False)\n",
    "HomeValuesUnfilterd = HomeValuesUnfilterd.set_index(\"RegionName\")\n",
    "HomeValues_Data = HomeValuesUnfilterd.drop(columns=[\"SizeRank\"])\n",
    "\n",
    "HomeValues_Data.head()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8c0bcf2a08a3a9cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Median Household Income:",
   "id": "93a39fb2cb410229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def simplify(col):\n",
    "    if not isinstance(col, str):\n",
    "        return col\n",
    "    if col in [\"GEO_ID\", \"NAME\", \"Geographic Area Name\"]:\n",
    "        return col\n",
    "\n",
    "    parts = col.split(\"!!\")\n",
    "    last = parts[-1].strip()\n",
    "    last = (last.replace(\"(\", \"\")\n",
    "                .replace(\")\", \"\")\n",
    "                .replace(\",\", \"\")\n",
    "                .replace(\"--\", \"_\")\n",
    "                .replace(\" \", \"_\"))\n",
    "    return last\n",
    "\n",
    "folder_path = \"data/Median Household Income\"\n",
    "columns = [2014,2015,2016,2017,2018,2019,2020,2021,2022,2023]\n",
    "\n",
    "MedianHouseIncome_Data = {}\n",
    "\n",
    "items_in_folder = sorted(os.listdir(folder_path))\n",
    "\n",
    "for item_name in items_in_folder:\n",
    "    item_full_path = os.path.join(folder_path, item_name)\n",
    "\n",
    "    try:\n",
    "        # Year\n",
    "        year = int(\"\".join(filter(str.isdigit, item_name))[:4])\n",
    "        if year not in columns:\n",
    "            continue\n",
    "\n",
    "        # Load entire CSV raw\n",
    "        df_raw = pd.read_csv(item_full_path, header=None)\n",
    "\n",
    "        # Detect header row ‚Üí row containing \"!!\"\n",
    "        header_row_index = df_raw.apply(lambda row: row.astype(str).str.contains(\"!!\").any(), axis=1).idxmax()\n",
    "\n",
    "        # Set header\n",
    "        df_raw.columns = df_raw.iloc[header_row_index]\n",
    "\n",
    "        # Drop all rows up to header row\n",
    "        df = df_raw.drop(index=list(range(header_row_index+1)))\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Drop GEO_ID column (first column always)\n",
    "        df = df.drop(columns=df.columns[0])\n",
    "\n",
    "        # Drop MOE columns\n",
    "        df = df.drop(columns=df.filter(regex=\"Margin of Error\").columns)\n",
    "\n",
    "        # Clean NAME column (which is now correctly labeled)\n",
    "        name_col = df.columns[0]\n",
    "        df[name_col] = df[name_col].str.replace(\", Louisiana\", \"\", regex=False)\n",
    "        df[name_col] = df[name_col].str.replace(\" Parish\", \"\", regex=False)\n",
    "\n",
    "        # Set index\n",
    "        df = df.set_index(name_col)\n",
    "\n",
    "        # Simplify column names\n",
    "        df.columns = [simplify(col) for col in df.columns]\n",
    "\n",
    "        # Reindex by parish list\n",
    "        df = df.reindex(parishes)\n",
    "\n",
    "        # Store\n",
    "        MedianHouseIncome_Data[year] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {item_name}: {e}\")\n",
    "\n",
    "print(MedianHouseIncome_Data)\n"
   ],
   "id": "2ed8a4ab0a07c51d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HOme Bedroom Values\n",
   "id": "228afef19d16c782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"data/Home Values Roomes\"   # <-- your folder\n",
    "\n",
    "# List of Louisiana parishes (cleaned)\n",
    "parishes = [\n",
    "    \"Acadia\", \"Allen\", \"Ascension\", \"Assumption\", \"Avoyelles\", \"Beauregard\",\n",
    "    \"Bienville\", \"Bossier\", \"Caddo\", \"Calcasieu\", \"Caldwell\", \"Cameron\",\n",
    "    \"Catahoula\", \"Claiborne\", \"Concordia\", \"DeSoto\", \"East Baton Rouge\",\n",
    "    \"East Carroll\", \"East Feliciana\", \"Evangeline\", \"Franklin\", \"Grant\",\n",
    "    \"Iberia\", \"Iberville\", \"Jackson\", \"Jefferson\", \"Jefferson Davis\",\n",
    "    \"Lafayette\", \"Lafourche\", \"LaSalle\", \"Lincoln\", \"Livingston\", \"Madison\",\n",
    "    \"Morehouse\", \"Natchitoches\", \"Orleans\", \"Ouachita\", \"Plaquemines\",\n",
    "    \"Pointe Coupee\", \"Rapides\", \"Red River\", \"Richland\", \"Sabine\",\n",
    "    \"St. Bernard\", \"St. Charles\", \"St. Helena\", \"St. James\",\n",
    "    \"St. John the Baptist\", \"St. Landry\", \"St. Martin\", \"St. Mary\",\n",
    "    \"St. Tammany\", \"Tangipahoa\", \"Tensas\", \"Terrebonne\", \"Union\",\n",
    "    \"Vermilion\", \"Vernon\", \"Washington\", \"Webster\", \"West Baton Rouge\",\n",
    "    \"West Carroll\", \"West Feliciana\", \"Winn\"\n",
    "]\n",
    "\n",
    "# MASTER DATA STORAGE\n",
    "HomeValues_LA = {}\n",
    "\n",
    "# Loop through every file\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "    print(\"Loading:\", filename)\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Clean region names\n",
    "    df[\"RegionName\"] = df[\"RegionName\"].str.replace(\" Parish\", \"\", regex=False)\n",
    "    df[\"RegionName\"] = df[\"RegionName\"].str.replace(\" County\", \"\", regex=False)\n",
    "\n",
    "    # Filter to Louisiana\n",
    "    df_LA = df[(df[\"StateName\"] == \"LA\") & (df[\"RegionType\"] == \"county\")]\n",
    "\n",
    "    # Extra filter to ensure only valid parishes remain\n",
    "    df_LA = df_LA[df_LA[\"RegionName\"].isin(parishes)]\n",
    "\n",
    "    # Drop metadata columns to keep only date columns\n",
    "    metadata_cols = [\"RegionID\", \"SizeRank\", \"RegionName\", \"RegionType\", \"StateName\",\n",
    "                     \"State\", \"Metro\", \"StateCodeFIPS\", \"MunicipalCodeFIPS\"]\n",
    "    date_cols = [col for col in df_LA.columns if col not in metadata_cols]\n",
    "\n",
    "    # Store cleaned dataframe\n",
    "    HomeValues_LA[filename] = df_LA.set_index(\"RegionName\")[date_cols]\n",
    "\n",
    "print(\"Loaded Louisiana home values for:\", len(HomeValues_LA), \"files\")\n",
    "\n",
    "\n"
   ],
   "id": "a523afa84b0f0f15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Combine Data\n",
   "id": "5951f375e0538fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def yearly_home_values(df):\n",
    "    df = df.copy()\n",
    "    df = df.T\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df['Year'] = df.index.year\n",
    "    yearly = df.groupby('Year').mean()\n",
    "    return yearly.T\n",
    "\n",
    "# Convert home value files to yearly\n",
    "HomeValues_LA_Yearly = {}\n",
    "for name, df in HomeValues_LA.items():\n",
    "    HomeValues_LA_Yearly[name] = yearly_home_values(df)\n",
    "\n",
    "# Mortgage yearly\n",
    "mort_yearly = (\n",
    "    HomeRatesUnfiltered\n",
    "        .set_index(\"observation_date\")\n",
    "        .resample(\"Y\")[\"MORTGAGE30US\"]\n",
    "        .mean()\n",
    "        .to_frame()\n",
    ")\n",
    "mort_yearly.index = mort_yearly.index.year\n",
    "mort_yearly.rename(columns={\"MORTGAGE30US\": \"MortgageRate\"}, inplace=True)\n",
    "\n",
    "# Crime yearly\n",
    "crime = crime_Data.copy()\n",
    "\n",
    "if crime.columns[0] == \"\":\n",
    "    crime = crime.drop(columns=crime.columns[0])\n",
    "\n",
    "crime_long = crime.melt(\n",
    "    id_vars=\"Parish\",\n",
    "    var_name=\"MonthYear\",\n",
    "    value_name=\"CrimeRate\"\n",
    ")\n",
    "\n",
    "crime_long[\"MonthYear\"] = pd.to_datetime(crime_long[\"MonthYear\"], format=\"%b-%y\")\n",
    "crime_long[\"Year\"] = crime_long[\"MonthYear\"].dt.year\n",
    "\n",
    "crime_yearly = (\n",
    "    crime_long.groupby([\"Parish\", \"Year\"])[\"CrimeRate\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# School yearly\n",
    "school_yearly = school_data.reset_index().melt(\n",
    "    id_vars=\"index\",\n",
    "    var_name=\"Year\",\n",
    "    value_name=\"School\"\n",
    ")\n",
    "school_yearly.rename(columns={\"index\": \"Parish\"}, inplace=True)\n",
    "school_yearly[\"Year\"] = school_yearly[\"Year\"].astype(int)\n",
    "\n",
    "# Income yearly\n",
    "income_frames = []\n",
    "for year, df in MedianHouseIncome_Data.items():\n",
    "    temp = df.reset_index()\n",
    "    temp = temp.iloc[:, :2]   # Parish + value\n",
    "    temp.columns = [\"Parish\", \"Income\"]\n",
    "    temp[\"Year\"] = year\n",
    "    income_frames.append(temp)\n",
    "\n",
    "income_yearly = pd.concat(income_frames, ignore_index=True)\n",
    "\n",
    "# Build base MASTER structure (1 row per Parish √ó Year √ó Bedroom)\n",
    "YEARS = list(range(2014, 2025))\n",
    "import itertools\n",
    "\n",
    "master = pd.DataFrame(list(itertools.product(parishes, YEARS)), columns=[\"Parish\", \"Year\"])\n",
    "\n",
    "# üî• ADD BEDROOM SUPPORT: Merge ALL bedroom datasets\n",
    "bedroom_dfs = []\n",
    "\n",
    "for name, df in HomeValues_LA_Yearly.items():\n",
    "    # Extract bedroom count from filename (e.g., \"3Bd.csv\" ‚Üí 3)\n",
    "    bedrooms = int(name.replace(\"Bd.csv\", \"\"))\n",
    "\n",
    "    df = df.copy()\n",
    "    df.index.name = \"Parish\"   # Fix index name\n",
    "\n",
    "    # Melt into long format: Parish | Year | HomeValue\n",
    "    long_df = df.reset_index().melt(\n",
    "        id_vars=\"Parish\",\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"HomeValue\"\n",
    "    )\n",
    "\n",
    "    # Convert Year column from string to integer\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "\n",
    "    # Add Bedrooms column\n",
    "    long_df[\"Bedrooms\"] = bedrooms\n",
    "\n",
    "    bedroom_dfs.append(long_df)\n",
    "\n",
    "# Combine all bedroom datasets\n",
    "all_homes = pd.concat(bedroom_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Now master will have 5√ó more rows once merged (1‚Äì5 bedrooms per year)\n",
    "master = master.merge(all_homes, on=[\"Parish\", \"Year\"], how=\"left\")\n",
    "\n",
    "# Merge Crime / School / Income / Mortgage\n",
    "master = master.merge(crime_yearly, on=[\"Parish\", \"Year\"], how=\"left\")\n",
    "master = master.merge(school_yearly, on=[\"Parish\", \"Year\"], how=\"left\")\n",
    "master = master.merge(income_yearly, on=[\"Parish\", \"Year\"], how=\"left\")\n",
    "\n",
    "mort_yearly[\"Year\"] = mort_yearly.index\n",
    "master = master.merge(mort_yearly, on=\"Year\", how=\"left\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "728bd2fd26797586",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ridding of Naan",
   "id": "2208effe84a74fc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_cols = [\"HomeValue\", \"CrimeRate\", \"School\", \"Income\", \"MortgageRate\"]\n",
    "\n",
    "for col in num_cols:\n",
    "    master[col] = pd.to_numeric(master[col], errors=\"coerce\")\n",
    "\n",
    "master = master.sort_values([\"Parish\", \"Bedrooms\", \"Year\"])\n",
    "# Compute the Louisiana statewide crime average per year\n",
    "state_year_crime = (\n",
    "    master.groupby(\"Year\")[\"CrimeRate\"].mean()\n",
    ")\n",
    "\n",
    "# Fill missing crime values with the year‚Äôs mean\n",
    "master[\"CrimeRate\"] = master.apply(\n",
    "    lambda row: state_year_crime[row[\"Year\"]] if pd.isna(row[\"CrimeRate\"]) else row[\"CrimeRate\"],\n",
    "    axis=1\n",
    ")\n",
    "for col in [\"CrimeRate\", \"School\", \"Income\"]:\n",
    "    master[col] = (\n",
    "        master.groupby(\"Parish\")[col]\n",
    "              .transform(lambda s: s.interpolate().bfill().ffill())\n",
    "    )\n",
    "master[\"MortgageRate\"] = (\n",
    "    master[\"MortgageRate\"]\n",
    "        .interpolate()\n",
    "        .bfill()\n",
    "        .ffill()\n",
    ")\n",
    "\n",
    "\n",
    "print(master.isna().sum())\n",
    "master = master.dropna(subset=[\"HomeValue\"])\n",
    "print(master.isna().sum())\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "master.to_csv(\"data/processed/master_dataset.csv\", index=False)\n"
   ],
   "id": "6000db18e6fd48a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PHASE ONE LSMT\n",
   "id": "67c348d0c0e3b17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# We assume \"master\" dataframe already exists in memory\n",
    "\n",
    "# Sort rows\n",
    "master = master.sort_values([\"Parish\", \"Year\"]).reset_index(drop=True)\n",
    "master = master.drop(columns=[\"Income\"]) #REMPVE FOR CATBOOST\n",
    "\n",
    "# Select only the columns needed for LSTM\n",
    "feature_cols = [\"HomeValue\", \"CrimeRate\", \"School\", \"MortgageRate\", \"Bedrooms\"] #ADD \"Income\" FOR CATBOOST\n",
    "\n",
    "# Debug check\n",
    "print(master.head())\n",
    "print(master.isna().sum())\n"
   ],
   "id": "bfbb5f0cfc5a5e48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SEQ_LEN = 10      # LSTM uses past 5 years\n",
    "PRED_HORIZON = 10  # predict 10 years ahead\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(master[feature_cols])\n"
   ],
   "id": "57d4c12bf3b6994a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_parish_sequences(df, seq_len=SEQ_LEN):\n",
    "    data = scaler.transform(df[feature_cols].values)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+seq_len][0])  # home value = index 0\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ],
   "id": "d9a8e5d2e942cfc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_all, y_all = [], []\n",
    "\n",
    "for parish in master[\"Parish\"].unique():\n",
    "    pdf = master[master[\"Parish\"] == parish].sort_values(\"Year\")\n",
    "\n",
    "    if len(pdf) > SEQ_LEN + 1:\n",
    "        X, y = create_parish_sequences(pdf)\n",
    "        if len(X) > 0:\n",
    "            X_all.append(X)\n",
    "            y_all.append(y)\n",
    "\n",
    "X_all = np.vstack(X_all)\n",
    "y_all = np.hstack(y_all)\n",
    "\n",
    "print(\"Total samples:\", len(X_all))\n"
   ],
   "id": "1f572f76346b588a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "split = int(0.8 * len(X_all))\n",
    "\n",
    "X_train = torch.FloatTensor(X_all[:split])\n",
    "y_train = torch.FloatTensor(y_all[:split]).unsqueeze(1)\n",
    "\n",
    "X_test = torch.FloatTensor(X_all[split:])\n",
    "y_test = torch.FloatTensor(y_all[split:]).unsqueeze(1)\n",
    "# Convert NumPy arrays ‚Üí Torch tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "\n",
    "X_test_t  = torch.FloatTensor(X_test)\n",
    "y_test_t  = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n"
   ],
   "id": "3eb0923329d4486b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HomeLSTM(nn.Module):\n",
    "    def __init__(self, input_size=len(feature_cols), hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "model = HomeLSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model\n"
   ],
   "id": "f92cc3483c21bb58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"X_train:\", type(X_train))\n",
    "print(\"X_train_t:\", type(X_train_t))\n",
    "print(\"y_train:\", type(y_train))\n",
    "print(\"y_train_t:\", type(y_train_t))\n",
    "print(\"X_test:\", type(X_test))\n",
    "print(\"X_test_t:\", type(X_test_t))\n",
    "print(\"y_test:\", type(y_test))\n",
    "print(\"y_test_t:\", type(y_test_t))\n"
   ],
   "id": "ba406cb5c5127915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(X_train_t)\n",
    "    loss = criterion(pred, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(X_test_t)\n",
    "            test_loss = criterion(test_pred, y_test_t)\n",
    "        print(f\"Epoch {epoch+1} | Train Loss {loss.item():.6f} | Test Loss {test_loss.item():.6f}\")\n"
   ],
   "id": "700ad031d97e370c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    test_pred = model(X_test_t)\n",
    "\n",
    "mae = torch.mean(torch.abs(test_pred - y_test_t)).item()\n",
    "rmse = torch.sqrt(torch.mean((test_pred - y_test_t)**2)).item()\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n"
   ],
   "id": "f8ea2e2fcb77bbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "actual = y_test_t[:300].squeeze().numpy()\n",
    "pred = test_pred[:300].squeeze().detach().numpy()\n",
    "\n",
    "plt.plot(actual, label=\"Actual\")\n",
    "plt.plot(pred, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "84fd358ab8822220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Save model\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"SEQ_LEN\": SEQ_LEN\n",
    "}, \"home_value_lstm.pth\")\n",
    "\n",
    "print(\"Model saved!\")\n"
   ],
   "id": "1ba4988279b6859",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#load model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.serialization\n",
    "\n",
    "torch.serialization.add_safe_globals([MinMaxScaler])\n",
    "\n",
    "checkpoint = torch.load(\"home_value_lstm.pth\", weights_only=False)\n",
    "\n",
    "loaded_model = HomeLSTM(\n",
    "    input_size=len(checkpoint[\"feature_cols\"])\n",
    ")\n",
    "loaded_model.load_state_dict(checkpoint[\"model_state\"])\n",
    "loaded_model.eval()\n",
    "\n",
    "scaler = checkpoint[\"scaler\"]\n",
    "feature_cols = checkpoint[\"feature_cols\"]\n",
    "SEQ_LEN = checkpoint[\"SEQ_LEN\"]\n",
    "\n",
    "print(\"Model loaded!\")\n"
   ],
   "id": "210c1a04be997e4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_future_value(\n",
    "    parish,\n",
    "    bedrooms,\n",
    "    current_home_value,\n",
    "    years_ahead,\n",
    "    master_df,\n",
    "    model,\n",
    "    scaler,\n",
    "    feature_cols,\n",
    "    SEQ_LEN\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict future home value for a given parish + bedrooms.\n",
    "    Future steps = years ahead (because dataset is yearly).\n",
    "\n",
    "    Returns: float future home value\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Filter data\n",
    "    pdf = master_df[\n",
    "        (master_df[\"Parish\"] == parish) &\n",
    "        (master_df[\"Bedrooms\"].astype(int) == int(bedrooms))\n",
    "    ].sort_values(\"Year\")\n",
    "\n",
    "    if len(pdf) < SEQ_LEN:\n",
    "        raise ValueError(f\"Not enough data for {parish} Bedroom={bedrooms} (need {SEQ_LEN} years).\")\n",
    "\n",
    "    # 2. Prepare last sequence\n",
    "    last_seq = pdf[feature_cols].tail(SEQ_LEN).values\n",
    "    last_seq_scaled = scaler.transform(last_seq)\n",
    "\n",
    "    # ---- Correct scaling of only the home_value ----\n",
    "    home_min = scaler.data_min_[0]\n",
    "    home_max = scaler.data_max_[0]\n",
    "\n",
    "    scaled_home = (current_home_value - home_min) / (home_max - home_max)\n",
    "    # but this formula is WRONG‚Äîcorrect is:\n",
    "\n",
    "    scaled_home = (current_home_value - home_min) / (home_max - home_min)\n",
    "\n",
    "    # Override home value in last timestep\n",
    "    seq = last_seq_scaled.copy()\n",
    "    seq[-1][0] = scaled_home\n",
    "\n",
    "    # 3. Predict recursively year-by-year\n",
    "    for _ in range(years_ahead):\n",
    "        inp = torch.FloatTensor(seq).unsqueeze(0)\n",
    "        pred_scaled = model(inp).item()\n",
    "\n",
    "        # next timestep\n",
    "        new_row = seq[-1].copy()\n",
    "        new_row[0] = pred_scaled\n",
    "        seq = np.vstack([seq[1:], new_row])\n",
    "\n",
    "    # 4. Convert last prediction back to real dollars\n",
    "    final_scaled = np.zeros((1, len(feature_cols)))\n",
    "    final_scaled[0, 0] = pred_scaled\n",
    "\n",
    "    future_value = scaler.inverse_transform(final_scaled)[0][0]\n",
    "\n",
    "    return future_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "future_price = predict_future_value(\n",
    "    parish=\"East Baton Rouge\",\n",
    "    bedrooms=3,\n",
    "    current_home_value=250000,\n",
    "    years_ahead=10,\n",
    "    master_df=master,\n",
    "    model=loaded_model,\n",
    "    scaler=scaler,\n",
    "    feature_cols=feature_cols,\n",
    "    SEQ_LEN=SEQ_LEN\n",
    ")\n",
    "\n",
    "print(f\"Predicted home value in 10 years: ${future_price:,.0f}\")\n"
   ],
   "id": "aef361f84a3c74d0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
